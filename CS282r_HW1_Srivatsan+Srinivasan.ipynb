{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> CS282r- HW1 - BASIC RL ALGORITHMS</H1>\n",
    "<H2> Submitted by SRIVATSAN SRINIVASAN, 18 Sep, 2017</H2>\n",
    "<H3> Collaborators : Camilo Fosco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Problem 1 - Gridworld</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><H4> Assumptions Made </H4></u>\n",
    "1. Q- Learning and SARSA have been evaluated with epsilon 0.1 and action -error probability 0.01. The $\\alpha$ values were chosen as per the given problem. \n",
    "2. RMax and Thompson Sampling were run with the aforementioned parameters. But they were run only for lesser number of trials( 5-15) trials instead of 50 since the MDP determination consumed lot of runtime for each trial since solving the MDP is costly.  \n",
    "3. The plots here are the cumulative error over episodes and cumulative rewards over total number of iterations, where each data point on y-axis represents the average value taken across trials. Plots with error bars for individual trials looked too cluttered.\n",
    "4. Penalty for hitting the wall is taken as -1 unless stated otherwise in the plots or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for SARSA </H3>\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_1.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_2.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_3.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_Iter_1.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_Iter_2.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_SARSA_Iter_3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for Q-Learning </H3>\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_1.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_2.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_3.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_Iter_1.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_Iter_2.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_QLearning_Iter_3.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><H3>Parametric Inferences for Q and SARSA</H3></u>\n",
    "From the plots of the cumulative rewards with respect to the epsiodes, we can learn the following observations.n Each algorithm is an exploration-exploitation model that is either trained model-based or without models.\n",
    "\n",
    "1. $\\alpha = \\frac{1}{i}$  : When $\\alpha$ is set at $\\frac{1}{i}$ with i as the current iteration, we see that model doesn't learn fast enough, as the learning rate in the later iterations go to pretty low values. Consequently, the optima achieved is  inconsistent as the model doesn't learn fast enough and good enough from its previous experiences. We can see that the model is getting closer to the actual rewards in the later iterations, but terminates before con. This is the worst of the three parameter set.\n",
    "2. $\\alpha = min(0.5,\\frac{10}{i}$ : When $\\alpha$ is set at $\\frac{1}{i}$ where i is the current iteration , we see that the model learns relatively faster and better than the previous version of $\\alpha$ simply because during the exploration phase it is able to learn faster and able to converge on the optimal solution. Also, because it is able to learn faster, the number of iterations needed per epsiode comes down with time. This presented the best parameter set for the given problem under the given constraints.\n",
    "3. $\\alpha = 0.1$ : When$\\alpha$ is set at 0.1, we see that the model again learns faster than 1.) and is able to converge upon  the optimal solution. The difference from 2.) is that the previous version had a high learning at the start of the iterations - For the first 100 iterations, it was able to learn much faster than this setting and hence was able to converge on an optimal policy faster. The later episodes mostly involved exploiting the optimal path to goal.\n",
    "4. <b><u>CONCLUSION</b></u>: With these observations, we can conclude that the system converges to an optimal solution when the learning rate is higher at the start of the learning process and it tails off in the later portions, so that the system can effectively explore the setup and learn the optimal solutions in the starting phases. Also, for this setup with a relatively non-existent action error probability, Q learning and SARSA algorithms are practically indistinguishable. I expect to see a difference between them in the Cliffworld algorithm though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for RMax </H3>\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_Iter_5.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_Iter_10.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_Iter_50.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_50.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_10.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_RMax_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><H3>Parametric Inferences for RMax</H3></u>\n",
    "The RMax parameters converge much faster than the model-free methods as it has a clear demarcation fo exploration time, where it understands the states, transitions and rewards and constructs a model out of them while optimizing for the best policy with the trained transitions. The model based approach works fairly in this case since the transitions and rewards are fairly deterministic and hence are easier to learn. The problem could have been complicated for model based methods if the rewards and transitions for a given action are highly stochastic as training a transition matrix with multiple (s,s') pairs for a given action from a state s is costly. Within the RMax settings, lower min visit count converges faster than the higher min visit count since it learns the model quicker because the transition matrix is updated more frequently and naturally, MDP gets solved with more optimal solutions faster( since transition matrices are updated quicker). Again, lower min visit count works in this case as the transitions for a given action are not largely stochastic and hence, even smaller visit counts predict the transition accurately. Same argument might not work for stochastic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for Thompson Sampling </H3>\n",
    "<img src = \"files/Documents/HW1/Grid_Thompson_1_10.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_Thompson_1_50.png\">\n",
    "<img src = \"files/Documents/HW1/Grid_Thompson_10_50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><H3>Parametric Inferences for Thompson Sampling</H3></u>\n",
    " Model that has a higher Dirichlet param with similar reward param means that the initial prior is strong and it takes quite a few observations before the posterior alters the distribution strongly. Hence, correspondingly it takes longer time to figure the optimal policy. For instance, we can see that Dirichlet([1,1,...]) converges relatively faster than Dirichlet([10,10,10,......]). Similarly, starting with max reward of 50 instead of 10 makes exploration last longer before exploitation as any unexplored state seems highly attractive with maximum rewards - an approach that parallels the intuition embraced by RMax. \n",
    "\n",
    "Thompson sampling was computationally expensive and hence could not be trained with the same rigor as the others. The cumulative error over the iterations is monotonically decreasing which means the algorithm is learning the optimal path with more and more iterations. Also, the convergence we see here might be slightly different from the other ones. We consistently keep sampling the transition matrix - a Dirichlet distribution over infinitely many samples provides averages that are exactly the same as the parameter vector, but the individual samples might deviate from it till many iterations are done to reduce variance. This makes an ideal candidate for cases with stochastic transitions and stochastic rewards. In case of deterministic systems, RMax seems to do better as explained earlier because of ther determinism in action-next state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H2> <u>Problem 2 - Cliffworld</u></H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Assumptions</H3>\n",
    "1. The learning rate was set at 0.1. \n",
    "2. All plots are plotted for three different epsilon values 0.05, 0.2 and 0.4.\n",
    "3. Q_Learning and SARSA were sampled for 20 different action error proababilities between 0 and 1. Each trial was subject to 100 \n",
    "epsiodes and 5000 iterations as earlier and also mean was taken across 10 trials.\n",
    "4. RMax and Thompson, being computationally expensive, were run for 5 action error probabailities between 0 and 1. There were only.\n",
    "5. Pit reward was set at -100.\n",
    "5 trials conducted which was averaged out. The plots will look much more discrete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for CliffWorld QLearning and SARSA</H3>\n",
    "<img src = \"files/Documents/HW1/Cliff_QLearning_1.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_QLearning_2.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_QLearning_3.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_SARSA_1.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_SARSA_2.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_SARSA_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Plots for CliffWorld RMAX and Thompson</H3>\n",
    "Note : RMax and Thompson were sampled only at 5 distinct points and not severall points as the earlier algorithms.\n",
    "<img src = \"files/Documents/HW1/Cliff_RMax_1.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_RMax_2.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_RMax_3.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_Thompson_1.png\">\n",
    "<img src = \"files/Documents/HW1/Cliff_Thompson_2.png\">\n",
    "Note : RMax and Thompson were sampled only at 5 distinct points and not severall points as the earlier algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>INFERENCES</H3>\n",
    "1. We see that the observed cumulative reward distribution and the reward collected over the last 100 iterations decereases as the action error probability increases in all algorithms.\n",
    "2. We also know that Q-Learning always converges to the most optimal policy and SARSA converges to epsilon-greedy policy,(which will also be the most optimal policy if every node has had enough iterations to be explored along the path). Because Q-learning always assumes that the best policy would ensue from the forthcoming step, it learns the shortened(though more error-prone) path from start to goal in the gridworld example whereas SARSA would learn a much safer path. Hence, Q-Learning fetches more rewards than SARSA for the same parameter set. \n",
    "3. We also observe that both Q-Learning and SARSA face slightly adverse effects with increasing values of $\\epsilon$. Since epsilon represents the extent to which we can explore greedily even after learning a policy(Remember learning rate is constatnt here), higher values of espilon would necessarily make the algorithm converge slower as it might have a tendency to explore another path even after most optimal path might have been reached along the course.\n",
    "4. We also observe that the model based algorithms are more robust to changes in action error probabilities than the model-free methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Majority Code used to solve both problems with minor modifications to settings in each case.</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "\n",
    "# created by us \n",
    "import gridworld\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "#   This is a very simple script to play with RL params in a gridworld    #\n",
    "# ----------------------------------------------------------------------- #\n",
    "# \n",
    "# The goal of this code is to give you a basic set of abstractions for\n",
    "# RL problems, and also let you quickly explore the effects of many\n",
    "# different RL-related parameters.  You're also welcome to create your\n",
    "# own domains!  For the homework assignment, you'll likely want to\n",
    "# wrap all of this in a script to run for all the domains, algorithms,\n",
    "# and algorithm settings... so just use this as your guide.\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "#   Different Domains    #\n",
    "# ---------------------- #\n",
    "# You can also create your own!  The interpretation of the different symbols is\n",
    "# the following:\n",
    "#\n",
    "# '#' = wall\n",
    "# 'o' = origin grid cell\n",
    "# '.' = empty grid cell\n",
    "# '*' = goal\n",
    "gridmaze = [   # HERE: Make this one bigger, probably!\n",
    "    '#########',\n",
    "    '#..#....#',\n",
    "    '#..#..#.#',\n",
    "    '#..#..#.#',\n",
    "    '#..#.##.#',\n",
    "    '#....*#.#',\n",
    "    '#######.#',\n",
    "    '#o......#',\n",
    "    '#########']\n",
    "\n",
    "cliffmaze = [\n",
    "    '#######', \n",
    "    '#.....#', \n",
    "    '#.##..#', \n",
    "    '#o...*#',\n",
    "    '#XXXXX#', \n",
    "    '#######']    \n",
    "\n",
    "# ----------------- #\n",
    "#   Key Functions   # \n",
    "# ----------------- #\n",
    "# The policy outputs the action for each states \n",
    "def policy( state , Q_table , action_count , epsilon=0 ):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.choice( action_count ) \n",
    "    else: \n",
    "        Q_per_state = Q_table[state,:]\n",
    "        maxQ_value = max(Q_per_state)       \n",
    "        maxQ_arg = np.argwhere( Q_per_state == maxQ_value ).flatten()      \n",
    "        action = np.random.choice(maxQ_arg)       \n",
    "    return action \n",
    "\n",
    "# Takes in count table and updates it\n",
    "def update_count_table( transition_count_table , transition_attempts_table, reward_value_table , state , action , new_state , reward ):\n",
    "    transition_count_table[state, new_state, action] += 1;  \n",
    "    transition_attempts_table[state, action] += 1;\n",
    "    reward_value_table[new_state] = reward;     \n",
    "    return transition_count_table, transition_attempts_table, reward_value_table \n",
    "\n",
    "# Takes in counts and builds an MDP using the RMAX approach (unseen rewards set\n",
    "# to rmax, use the empirical counts for the transition frequencies)\n",
    "def build_MDP_RMAX( transition_matrix, transition_count_table , transition_attempts_table, reward_value_table, max_attempts, rmax ):\n",
    "    for state, count_row in enumerate(transition_attempts_table):      \n",
    "        for action, count in enumerate(count_row):          \n",
    "            if count > max_attempts: # Statistical Significance\n",
    "                for new_state in range(transition_matrix.shape[1]):                    \n",
    "                    transition_matrix[state,new_state,action] = transition_count_table[state,new_state,action]/transition_attempts_table[state,action]; \n",
    "    return transition_matrix\n",
    "\n",
    "# Takes in counts and samples and MDP \n",
    "def sample_MDP( transition_count_table , dirichlet_alpha ):\n",
    "     for state in range(state_count):\n",
    "            for action in range(action_count):\n",
    "                transition_matrix[state,:,action] = np.random.dirichlet(dirichlet_alpha+transition_count_table[state,:,action])            \n",
    "     return transition_matrix   \n",
    "\n",
    "# Solve MDP\n",
    "def solve_MDP( transition_matrix, reward_value_table, gamma, state_count, action_count):\n",
    "    V = np.zeros(state_count)\n",
    "    epsilon = 10 ** -3\n",
    "    dif = 10 ** 4 #Arbitrarily high number\n",
    "    while dif >= epsilon :        \n",
    "        Vnew=np.zeros(state_count);\n",
    "        for s in range(state_count):\n",
    "            for a in range(action_count):\n",
    "                value_per_action = np.dot(transition_matrix[:,:,a],(reward_value_table+gamma*V));  # Calculate value for each action\n",
    "                if value_per_action[s] > Vnew[s]:\n",
    "                    Vnew[s] = value_per_action[s]\n",
    "        dif = np.linalg.norm(Vnew-V);\n",
    "        V = Vnew;\n",
    "        \n",
    "    for a in range(action_count):        \n",
    "        Q_table[:,a] = np.dot(transition_matrix[:,:,a],(reward_value_table+gamma*V));\n",
    "        \n",
    "    return Q_table, V \n",
    "\n",
    "# Update the Q table \n",
    "def update_Q_SARSA( Q_table , alpha , gamma , state , action , reward , new_state , new_action ):\n",
    "    # FILL THIS IN\n",
    "    Q_table[state,action] = Q_table[state,action]+alpha*( reward + gamma *Q_table[new_state, new_action] - Q_table[state, action])\n",
    "    return Q_table \n",
    "\n",
    "# Update the Q table \n",
    "def update_Q_Qlearning( Q_table , alpha , gamma , state , action , reward , new_state , new_action ):\n",
    "    # FILL THIS IN\n",
    "    max_q = max(Q_table[new_state,:])\n",
    "    Q_table[state][action] = Q_table[state, action] + alpha *(reward + gamma * max_q - Q_table[state][action])\n",
    "    return Q_table \n",
    "\n",
    "def evaluate_Qlearning(Q_table, elem, task_iter, task_iter_sa, gamma, state, action, reward, new_state, new_action):\n",
    "    alpha = calculateAlphaFromMode(elem,task_iter,task_iter_sa)\n",
    "    Q_table = update_Q_Qlearning(Q_table, alpha, gamma, state, action, reward, new_state, new_action ) \n",
    "    return Q_table\n",
    "\n",
    "def evaluate_SARSA(Q_table, elem, task_iter, task_iter_sa, gamma, state, action, reward, new_state, new_action):\n",
    "    alpha = calculateAlphaFromMode(elem,task_iter,task_iter_sa)\n",
    "    Q_table = update_Q_SARSA(Q_table, alpha, gamma, state, action, reward, new_state, new_action ) \n",
    "    return Q_table\n",
    "# -------------------- #\n",
    "#   Create the Task    #\n",
    "# -------------------- #\n",
    "# Task Parameters for gridworld HHHH this is where I'm still editing \n",
    "task_name = gridmaze\n",
    "action_error_prob = .01\n",
    "pit_reward = -50\n",
    "task = gridworld.GridWorld( gridworld.Maze(task_name) ,\n",
    "                            action_error_prob=action_error_prob, \n",
    "                            rewards={'*': 50, 'moved': -1, 'hit-wall': -1,'X':pit_reward} ,\n",
    "                            terminal_markers='*' )     \n",
    "rmax = task.get_max_reward()   \n",
    "'''\n",
    "# Task Parameters for gridworld \n",
    "task_name = cliffmaze\n",
    "action_error_prob = .01\n",
    "pit_reward = -50\n",
    "task = gridworld.GridWorld( task_name ,\n",
    "                            action_error_prob=action_error_prob, \n",
    "                            rewards={'*': 50, 'moved': -1, 'hit-wall': -1,'X':pit_reward} ,\n",
    "                            terminal_markers='*' )        \n",
    "'''\n",
    "rmax = task.get_max_reward()\n",
    "\n",
    "def calculateAlphaFromMode(alpha_mode, overall_iteration_count, sa_iteration_count):\n",
    "    if alpha_mode == 1 :\n",
    "        alpha = 1 / sa_iteration_count\n",
    "    elif alpha_mode == 2:\n",
    "        alpha = min(0.5, 10/sa_iteration_count) \n",
    "    elif alpha_mode == 3:\n",
    "        alpha = 1/ overall_iteration_count\n",
    "    elif alpha_mode == 4:\n",
    "        alpha = min(0.5, 10/overall_iteration_count)        \n",
    "    else:\n",
    "        alpha = 0.1\n",
    "    return min(1,max(0,alpha))\n",
    "\n",
    "def calculateAlphaString(alpha_mode):\n",
    "    if alpha_mode == 1 : \n",
    "        alpha_string = \"1/Current Iteration\"\n",
    "    elif alpha_mode == 2:\n",
    "        alpha_string = \"min(0.5, 10/Current Iteration)\"\n",
    "    elif alpha_mode == 3: \n",
    "        alpha_string = \"1/Overall Current Iteration\"\n",
    "    elif alpha_mode == 4:\n",
    "        alpha_string = \"min(0.5, 10/Overall Current Iteration)\"\n",
    "    else:\n",
    "        alpha_string = \"0.1\"\n",
    "    return(alpha_string)\n",
    "\n",
    "# ---------------- #\n",
    "#   Run the Task   # \n",
    "# ---------------- #\n",
    "# Algorithm Parameters \n",
    "\n",
    "epsilon = 0.1\n",
    "gamma = .95\n",
    "state_count = task.num_states  \n",
    "action_count = task.num_actions \n",
    "episode_count = 220\n",
    "episode_max_count = 100\n",
    "rep_count = 50\n",
    "rep_count_map = {\"QLearning\":50, \"SARSA\":50, \"RMax\":2, \"Thompson\":2}\n",
    "max_task_iter = 5000\n",
    "rmax_recompute_iter = 50\n",
    "algo_list = [\"Thompson\" ]\n",
    "algo_param_num = {\"QLearning\":5, \"SARSA\":5, \"RMax\":3, \"Thompson\":3}\n",
    "overall_reward_map = {}\n",
    "V = np.zeros(state_count)\n",
    "\n",
    "#Q and SARSA\n",
    "alpha_mode_list = [1,2,5]\n",
    "#RMax\n",
    "RMax_attempts = [5,10,50]\n",
    "#Dirichlet\n",
    "Dirichlet_Param_Set = [[1,50],[1,10],[10,50]]\n",
    "\n",
    "param_map = {\"QLearning\" : alpha_mode_list, \"SARSA\" : alpha_mode_list, \"RMax\" : RMax_attempts, \"Thompson\" : Dirichlet_Param_Set }\n",
    "\n",
    "for algorithm in algo_list :\n",
    "    rep_count = rep_count_map[algorithm]\n",
    "    param_iter = 0    \n",
    "    overall_reward_map[algorithm] = {}\n",
    "    # Loop over some number of episodes\n",
    "    for elem in param_map[algorithm]:\n",
    "        param_iter += 1\n",
    "        episode_reward_set = np.zeros( (rep_count , 10 * episode_max_count ))  \n",
    "        iter_reward_set = np.zeros((rep_count, 50*max_task_iter))\n",
    "        reward_iter_sum = 0\n",
    "        episode_counter =[]\n",
    "        iter_counter=[]\n",
    "        for rep_iter in range( rep_count ):    \n",
    "            # Initialize the Q table \n",
    "            Q_table = np.zeros( ( state_count , action_count ) )\n",
    "            transition_count_table = np.zeros( ( state_count , state_count, action_count ) )\n",
    "            transition_attempts_table = np.zeros( (state_count, action_count) )                \n",
    "            \n",
    "            r_init = 1 #Trivial for Q and SARSA\n",
    "            \n",
    "            if algorithm == \"RMax\":\n",
    "                MDP_eval_iter = 0\n",
    "                r_init = rmax\n",
    "                transition_matrix =  np.ones( ( state_count , state_count, action_count ) ) / state_count\n",
    "            \n",
    "            if (algorithm == 'Thompson'):        \n",
    "                r_init = elem[1]\n",
    "                dirichlet_alpha = np.ones(state_count)*elem[0]\n",
    "                transition_matrix =  sample_MDP(transition_count_table, np.ones(state_count)*elem[0])\n",
    "                \n",
    "            reward_value_table = np.ones( ( state_count ) )*r_init\n",
    "            \n",
    "            episode_iter = 0\n",
    "            task_iter = 0\n",
    "            episode_over = False\n",
    "            task_iter_sa = [[0 for i in range(action_count)]for j in range(state_count)]\n",
    "            \n",
    "            # Loop until the episode is done \n",
    "            while True:\n",
    "                episode_iter += 1\n",
    "                # Start the task \n",
    "                task.reset()\n",
    "                state = task.observe() \n",
    "                action = policy( state , Q_table , action_count , epsilon ) \n",
    "                episode_reward_list = []     \n",
    "        \n",
    "                # Loop until done\n",
    "                while True:\n",
    "                    task_iter = task_iter + 1 \n",
    "                    new_state, reward = task.perform_action( action )\n",
    "                    new_action = policy( new_state , Q_table , action_count , epsilon ) \n",
    "                    task_iter_sa[state][action] += 1                  \n",
    "                                             \n",
    "                    ################SARSA################                                                          \n",
    "                    if algorithm == \"SARSA\":\n",
    "                        Q_table = evaluate_SARSA(Q_table, elem, task_iter, task_iter_sa[state][action], \n",
    "                                                 gamma, state, action, reward, new_state, new_action)                       \n",
    "                    ################Q_LEARNING##############    \n",
    "                    elif algorithm == \"QLearning\":                      \n",
    "                        Q_table = evaluate_Qlearning(Q_table, elem, task_iter, task_iter_sa[state][action], \n",
    "                                                 gamma, state, action, reward, new_state, new_action)\n",
    "                    ################RMAX#################    \n",
    "                    elif algorithm == \"RMax\":\n",
    "                        MDP_eval_iter += 1\n",
    "                        transition_count_table, transition_attempts_table, reward_value_table = update_count_table(transition_count_table, transition_attempts_table, reward_value_table, state, action, new_state, reward)\n",
    "                        if MDP_eval_iter >= rmax_recompute_iter:\n",
    "                            MDP_eval_iter = 0\n",
    "                            transition_matrix = build_MDP_RMAX(transition_matrix,transition_count_table, transition_attempts_table, reward_value_table, elem, rmax)\n",
    "                            Q_table, V = solve_MDP(transition_matrix, reward_value_table, gamma, state_count, action_count)\n",
    "                    #################THOMPSON####################       \n",
    "                    elif algorithm == \"Thompson\":\n",
    "                        MDP_eval_iter += 1\n",
    "                        transition_count_table, transition_attempts_table, reward_value_table = update_count_table(transition_count_table, transition_attempts_table, reward_value_table, state, action, new_state, reward)\n",
    "                        if MDP_eval_iter >= rmax_recompute_iter:\n",
    "                            MDP_eval_iter = 0\n",
    "                            transition_matrix = sample_MDP( transition_count_table ,dirichlet_alpha );                            \n",
    "                            Q_table, V = solve_MDP(transition_matrix, reward_value_table,  gamma, state_count, action_count)\n",
    "                                                                                                                                                                                                   \n",
    "                    # store the data               \n",
    "                    episode_reward_list.append( reward )   \n",
    "                    iter_reward_set[rep_iter, task_iter] = reward\n",
    "                    reward_iter_sum += reward\n",
    "                   # iteration_reward_set[rep_iter, task_iter] = reward_iter_sum\n",
    "                    # stop if at goal/else update for the next iteration \n",
    "                    if task.is_terminal( new_state ):\n",
    "                        break\n",
    "                    else:\n",
    "                        state = new_state\n",
    "                        action = new_action\n",
    "                    #print(\"Iter OVer\")\n",
    "        \n",
    "                    episode_over = (episode_iter >= episode_max_count and task_iter >= max_task_iter)\n",
    "                    \n",
    "                    if episode_over:   \n",
    "                      \n",
    "                        break                      \n",
    "                # Store the rewards  \n",
    "                                               \n",
    "                episode_reward_set[ rep_iter , episode_iter ] = np.sum( episode_reward_list )\n",
    "                print(\"Episode Over\"+str(episode_iter))\n",
    "               # episode_reward_set[ rep_iter][ episode_iter ] = np.sum( episode_reward_list )\n",
    "                if episode_over:                    \n",
    "                   break\n",
    "            iter_counter.append(task_iter)   \n",
    "            episode_counter.append(episode_iter)\n",
    "               \n",
    "        overall_reward_map[algorithm][str(param_iter)] = episode_reward_set\n",
    "   \n",
    "        \n",
    "        # -------------- #\n",
    "        #   Make Plots   #\n",
    "        # -------------- #\n",
    "        # Note, these are plots that are useful for visualizing the policies\n",
    "        # and the value functions, which can help you identify bugs.  You can\n",
    "        # also use them as a starting point to create the plots that you will\n",
    "        # need for your homework assignment.\n",
    "        \n",
    "        # Util to make an arrow \n",
    "        # The directions are [ 'north' , 'south' , 'east' , 'west' ] \n",
    "        def plot_arrow( location , direction , plot ):\n",
    "        \n",
    "            arrow = plt.arrow( location[0] , location[1] , dx , dy , fc=\"k\", ec=\"k\", head_width=0.05, head_length=0.1 )\n",
    "            plot.add_patch(arrow) \n",
    "        \n",
    "        # Useful stats for the plot\n",
    "        row_count = len( task_name )\n",
    "        col_count = len( task_name[0] ) \n",
    "        value_function = np.reshape( np.max( Q_table , 1 ) , ( row_count , col_count ) )\n",
    "        policy_function = np.reshape( np.argmax( Q_table , 1 ) , ( row_count , col_count ) )\n",
    "        wall_info = .5 + np.zeros( ( row_count , col_count ) )\n",
    "        wall_mask = np.zeros( ( row_count , col_count ) )\n",
    "        for row in range( row_count ):\n",
    "            for col in range( col_count ):\n",
    "                if task_name[row][col] == '#':\n",
    "                    wall_mask[row,col] = 1     \n",
    "        wall_info = np.ma.masked_where( wall_mask==0 , wall_info )\n",
    "        \n",
    "        plt.close()\n",
    "        # Plot the rewards\n",
    "        plt.subplot( 3 , 1 , 1 ) \n",
    "       \n",
    "        mean_episode_error = np.mean(episode_reward_set, axis=0 )\n",
    "        mean_episode_error = mean_episode_error[0:max(episode_counter)-1]\n",
    "        plt.plot(mean_episode_error) \n",
    "        plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "       \n",
    "        if algorithm in [\"QLearning\", \"SARSA\"]:\n",
    "            alpha_string = calculateAlphaString(elem)      \n",
    "            plt.title( 'Cum.reward per episode in algo ' + algorithm + ' with alpha = ' + alpha_string ) \n",
    "        \n",
    "        if algorithm in [\"RMax\"]:\n",
    "            mincountstring = str(elem)   \n",
    "            plt.title( 'Cum.reward per episode in algo ' + algorithm + ' with mincount = ' + mincountstring ) \n",
    "            \n",
    "        if algorithm in [\"Thompson\"]:\n",
    "            mincountstring = str(elem[0]) + ',' + str(elem[1])   \n",
    "            plt.title( 'Cum.reward per episode in algo ' + algorithm + ' with initial setting of (dirichlet,reward ) = ' + mincountstring )     \n",
    "            \n",
    "        plt.xlabel( 'Episode Number' )\n",
    "        plt.ylabel( 'Sum of Rewards in Episode' )\n",
    "        \n",
    "        # Plot the rewards\n",
    "        plt.subplot( 3, 1, 2) \n",
    "       \n",
    "        mean_iter_error = np.mean(iter_reward_set, axis=0 )\n",
    "        mean_iter_error = mean_iter_error[0:max(iter_counter)-1]\n",
    "        mean_iter_error = np.cumsum(mean_iter_error)\n",
    "        plt.plot(mean_iter_error) \n",
    "        plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "       \n",
    "        if algorithm in [\"QLearning\", \"SARSA\"]:\n",
    "            alpha_string = calculateAlphaString(elem)      \n",
    "            plt.title( 'Cum.reward per iteration in algo ' + algorithm + ' with alpha = ' + alpha_string ) \n",
    "        \n",
    "        if algorithm in [\"RMax\"]:\n",
    "            mincountstring = str(elem)   \n",
    "            plt.title( 'Cum.reward per iteration in algo ' + algorithm + ' with mincount = ' + mincountstring ) \n",
    "            \n",
    "        if algorithm in [\"Thompson\"]:\n",
    "            mincountstring = str(elem[0]) + ',' + str(elem[1])   \n",
    "            plt.title( 'Cum.reward per iteration in algo ' + algorithm + ' with initial setting of (dirichlet,reward ) = ' + mincountstring )     \n",
    "            \n",
    "        plt.xlabel( 'Iteration Number' )\n",
    "        plt.ylabel( 'Cum. Rewards in Iteration' )          \n",
    "                 \n",
    "        # value function plot \n",
    "        plt.subplot( 3, 1, 3 ) \n",
    "        plt.imshow( value_function , interpolation='none' , cmap=matplotlib.cm.jet )\n",
    "        plt.colorbar()\n",
    "        plt.imshow( wall_info , interpolation='none' , cmap=matplotlib.cm.gray )\n",
    "        plt.title( 'Value Function' )\n",
    "       # plt.figure(figsize=(5,5))\n",
    "        \n",
    "        # policy plot \n",
    "        # plt.imshow( 1 - wall_mask , interpolation='none' , cmap=matplotlib.cm.gray )    \n",
    "        for row in range(row_count):\n",
    "            for col in range( col_count ):\n",
    "                if wall_mask[row][col] == 1:\n",
    "                    continue \n",
    "                if policy_function[row,col] == 0:\n",
    "                    dx = 0; dy = -.5\n",
    "                if policy_function[row,col] == 1:\n",
    "                    dx = 0; dy = .5\n",
    "                if policy_function[row,col] == 2:\n",
    "                    dx = .5; dy = 0\n",
    "                if policy_function[row,col] == 3:\n",
    "                    dx = -.5; dy = 0\n",
    "                plt.arrow( col , row , dx , dy , shape='full', fc='w' , ec='w' , lw=3, length_includes_head=True, head_width=.2 )\n",
    "#        plt.title( 'Policy' )    \n",
    "        plt.tight_layout()    \n",
    "        plt.show( block=False ) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
